{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60308fb3-4c29-4a4f-919d-46d93a140c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b332bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f34b7-745e-4311-89de-eec634c3a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#w2v_model = gensim.models.Word2Vec.load('word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_sentences = [\"Access control procedures mentioned in the document include:\\n\\n1. On page 60-62 (GCP SOC 2 doc): Customers are responsible for provisioning, maintaining, monitoring, and disabling end users' access according to their internal access management policies. The entity implements logical access security software, infrastructure, and architectures over protected information assets to protect them from security events that meet the entity's objectives. Prior to issuing system credentials and granting system access, the entity registers and authorizes new internal and external users whose access is administered by the entity. For those users whose access is administered by the entity, user system credentials are removed when access is no longer authorized.\\n\\n2. On page 67 (GCP SOC 2 doc): The entity authorizes, modifies, or removes access to data, software, functions, and other protected information assets based on roles. This is done within the customer's environment consistent with customer policies.\\n\\n3. On page 48 (GCP SOC 2 doc): Production system access is granted only to individuals who have completed required security and privacy training and require this level of access to perform tasks. Access to individual production systems via critical access groups is reviewed periodically by the system owners, and inappropriate access is removed for Google personnel who no longer have a business need for it. Access to all corporate and production resources are automatically removed upon submission of a termination request by the manager of any departing employee or by the appropriate Human Resources manager.\\n\\n4. Password guidelines: Google personnel are required to authenticate using valid credentials prior to resetting their passwords. Passwords are managed in accordance with a set of password construction requirements (cited from page 48, but the exact guidelines are not specified in the given context).\",\n",
    "                \"A backup of the data is performed as per the data retention and deletion policies. The process involves the use of data deletion tools that verify the backup data is deleted following the configured retention period, as part of the deletion mechanism process. This information can be found on page 77 of the document: 'was disposed of as per the data retention and deletion policies. No deviations noted. Inspected a sample product and determined data deletion tools verified that backup data was deleted following the configured retention period, as part of the deletion mechanism process. No deviations noted.'\",\n",
    "                \"To ensure that periodic penetration tests are performed for the infrastructure, devices, and end-points, Google has the following measures in place:\\n\\n1. Penetration tests are performed at least annually. (CC4.1)\\n2. The organization performs penetration tests by qualified internal personnel or an external service provider at least annually. (Inquired of the Program Manager)\\n\\nNo deviations were noted in these procedures during the testing process, as confirmed by the results shared by EY after their inspection. \\n\\nReference(s):\\n- Page 54: 'logs is restricted to authorized personnel. Security event logs are monitored continuously using a Google proprietary Security Event Management (SEM) system to detect intrusion attempts and other security related events.'\\n- Page 168: 'Control Description SOC 2 Criteria, Controls, Tests and Results of Tests \\\\n...112. Penetration tests are performed at least annually. CC4.1...'\"]\n",
    "\n",
    "#answers to questions 1, 15, and 17\n",
    "embeddings_AI = model.encode(AI_sentences)\n",
    "\n",
    "\n",
    "for sentence, embedding in zip(AI_sentences, embeddings_AI):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ba2d7-80ea-4cee-b495-8a6e0b3f3857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answers to questions 1, 15, and 17\n",
    "\n",
    "TR_sentences = [\"Google follows a formal process to grant or revoke employee access to Google resources. Access to systems and data is granted only to authorized users. Access requests are reviewed and approved by an authorized second individual prior to being granted and the event is logged. Both user and internal access to customer data are restricted through the use of unique user account IDs and the Google Accounts Bring Your Own Identity (BYOID) system. Access to sensitive systems and applications requires two-factor authentication. Periodic reviews of access lists are implemented to ensure access to customer data is appropriate and authorized. Access to production machines, network devices and support tools is managed via an access group management system. Membership in these groups must be approved by respective group administrators. User group memberships are reviewed on a semiannual basis and any inappropriate access is removed. Access authorization in Google Cloud Platform is enforced at all relevant layers of the system. The granting or modification of access rights is based on the user's job responsibilities or on a need-to-know basis and must be authorized and approved by the user's functional manager or system owners. Access to all corporate and production resources are automatically removed upon submission of a termination request by the manager of any departing employee, or by the appropriate Human Resources manager.\",\n",
    "                \"At Google Cloud Platform, data backup is performed through a robust, automated system that ensures data integrity and availability. We use a combination of incremental and full backups to optimize both storage efficiency and data recovery times. These backups are geographically distributed across multiple secure data centers to provide redundancy and high availability. Additionally, our backup procedures are compliant with industry standards and are regularly audited to ensure they meet stringent security and privacy requirements.\",\n",
    "                \"External third-party penetration tests are performed on an annual basis for a predetermined subset of the services included in the Google Cloud Platform System. The subset of services included in any given year are determined by the Google Security and the Office of Compliance & Integrity teams. This is based on their understanding of the organization's current risk environment, as well as the organization's current regulatory and compliance requirements. Corrective actions are taken as necessary.\"]\n",
    "\n",
    "\n",
    "embeddings_TR = model.encode(TR_sentences)\n",
    "\n",
    "\n",
    "for sentence, embedding in zip(TR_sentences, embeddings_TR):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff63fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"What access control procedures are in place?\",\n",
    "             \"How is a back up of the data performed?\",\n",
    "             \"What is done to ensure that periodic penetration tests are performed for the infrastructure, devices, and end-points?\"]\n",
    "\n",
    "question_embeddings = model.encode(questions)\n",
    "\n",
    "\n",
    "for sentence, embedding in zip(questions, question_embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d71df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_sentences_ai = [ai.split() for ai in AI_sentences]\n",
    "# tokenized_sentences_tr = [tr.split() for tr in TR_sentences]\n",
    "# tokenized_sentences = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_embeddings = []\n",
    "# for sentence in tokenized_sentences:\n",
    "#     sentence_embeddings = []\n",
    "#     for token in sentence:\n",
    "#         if token in w2v_model.wv.vocab:\n",
    "#             sentence_embeddings.append(w2v_model.wv[token])\n",
    "#     word_embeddings.append(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c11fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_embeddings = []\n",
    "# for sentence in word_embeddings:\n",
    "#     sentence_embedding = np.mean(sentence, axis=0)\n",
    "#     sentence_embeddings.append(sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute cosine similarity\n",
    "# cos_sims = [util.cos_sim(ai, tr) for ai, tr in zip(sentence_embeddings[:len(AI_sentences)], sentence_embeddings[len(AI_sentences):])]\n",
    "# print(\"Cosine-Similarities:\", cos_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1331ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sims = [util.cos_sim(ai, tr) for ai,tr in zip(embeddings_AI, embeddings_TR)]\n",
    "print(\"Cosine-Similarities:\", cos_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a166340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `cos_sims` is a PyTorch tensor containing the cosine similarities\n",
    "cos_percent = torch.tensor([i for i in cos_sims])\n",
    "\n",
    "# Convert cosine similarities to percentages\n",
    "percentages = (cos_percent + 1) / 2 * 100\n",
    "\n",
    "print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4faf72-c27b-4b4a-9e47-2f8e57b36842",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = model.encode(\"I am eating Apple\")\n",
    "emb2 = model.encode(\"I like fruits\")\n",
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cosine-Similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e41e6-bf56-4b19-9ac9-74d7d49b87fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = model.encode(\"The quick brown fox jumps over the lazy dog\")\n",
    "emb2 = model.encode(\"The fast brown fox leaps over the lazy dog\")\n",
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cosine-Similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab32a1ed-3a96-47f7-ba76-ba14f199e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = model.encode(\"The quick brown fox jumps over the lazy dog\")\n",
    "emb2 = model.encode(\"I like fruits\")\n",
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cosine-Similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732cabc-dd91-4173-932d-da2d9f616d64",
   "metadata": {},
   "source": [
    "**Bert Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f55d7d",
   "metadata": {},
   "source": [
    "The current attempt is to use bert score to help provide context to what we are trying to find similarity on. Currently the process takes in the responses, questions, and pdf evidence. We then set up out knowledge graph centered around the pdf document. This is using Neo4j and requires the community version download. The idea is that with the knowledge graph we can help limit the parameters of similarity to focus on all that the evidence doc encompasses. Once we have the questions, entities, and relationships available we can then apply them to the sentences that will be compared thus allowing for further levels of consideration when generating the bert score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c030a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_score\n",
    "import spacy\n",
    "# Create a knowledge graph using a graph database like Neo4j\n",
    "import neo4j\n",
    "import faiss\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f56a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(\"C:/Users/ansutton/Desktop/TPRM/TPRM-Accelerator/assets/data/Security Evidence Docs/SOC 2/GoogleCloud/Audit-Reports-1720774833381-81ba2e/GCP-[FALL-2023] GCP SOC 2..pdf\")\n",
    "documents = pdf_loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SpaCy model for entity recognition\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce3813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from the PDF document\n",
    "text = \" \".join([doc.page_content for doc in documents])\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [(ent.text, ent.label_) for ent in nlp(doc).ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe4736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relationships using depemndency parsing\n",
    "relationships = []\n",
    "for sentence in doc.sents:\n",
    "    for token in sentence:\n",
    "        if token.dep_ == \"nsubj\" or token.dep_ == \"dobj\":\n",
    "            relationships.append((token.text, token.head.text, token.dep_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20955394",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = neo4j.GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"Indiana2019\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277412a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a knowledge graph using the extracted entities and relationships\n",
    "with driver.session() as session:\n",
    "    for entity in entities:\n",
    "        session.run(\"CREATE (e:Entity {name: $name, label: $label})\", name=entity[0], label=entity[1])\n",
    "    for relationship in relationships:\n",
    "        session.run(\"MATCH (e1:Entity {name: $entity1}), (e2:Entity {name: $entity2}) CREATE (e1)-[:RELATED_TO {type: $type}]->(e2)\", entity1=relationship[0], entity2=relationship[1], type=relationship[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_scorer = bert_score.BERTScorer(lang='en', rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant entities and relationships from the knowledge graph\n",
    "with driver.session() as session:\n",
    "    entities_and_relationships = []\n",
    "    for question in questions:\n",
    "        entities_and_relationships.extend(session.run(\"MATCH (e:Entity)-[:RELATED_TO]-(r:Entity) WHERE e.name = $question RETURN e, r\", question=question).data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399cd3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a knowledge graph-based context\n",
    "kg_context = []\n",
    "for entities_and_relationship in entities_and_relationships:\n",
    "    kg_context.append(\" \".join([entity[\"e\"][\"name\"] for entity in entities_and_relationship] + [relationship[\"r\"][\"type\"] for relationship in entities_and_relationship]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b065b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the knowledge graph context with the input sentences\n",
    "AI_sentences_with_context = [f\"{kg_context[i]} {question} {sentence}\" for i, (question, sentence) in enumerate(zip(questions, AI_sentences))]\n",
    "TR_sentences_with_context = [f\"{kg_context[i]} {question} {sentence}\" for i, (question, sentence) in enumerate(zip(questions, TR_sentences))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = bert_scorer.score(AI_sentences_with_context, TR_sentences_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8138e745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
